{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "J3NzmACPjF0g",
    "nbgrader": {
     "checksum": "e9415216345b6fd30b4cd117eddc5d31",
     "grade": false,
     "grade_id": "cell-da7e5eaae3b1d9a5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Zomato Recommendation Engine\n",
    "\n",
    "Make a restaurant recommendation engine for Zomato using user review data.\n",
    "\n",
    "![alt text](zomato-banner.png)\n",
    "\n",
    "## Prologue\n",
    "\n",
    "You have been lucky and after graduation got employed as a Data Engineer at Zomato. You have been living in Mumbai, spending your big fat check, but due to the recent emergence of Swiggy, Zomato is now facing a major competitor in the market. Your boss Dr Mukherjee, given the situation, has assigned you to improve the recommendation engine employed at Zomato. It is now upon you to keep your stocks floating and grab that year-end bonus.\n",
    "\n",
    "It has been two days since you were assigned the project, you researched about recommender systems, for which you found [this blog](https://towardsdatascience.com/brief-on-recommender-systems-b86a1068a4dd) extremely helpful. After reading the blog, you decided to make a **collaborative filtering based recommendation system using autoencoders**, since Zomato was already using content-based filtering which wasn't giving promising results. You decided to use user review data on restaurants situated in Mumbai since it has a high user engagement. Following are the notes, you made while researching:\n",
    "\n",
    "<!-- In this assignment, we will be working on implementing a recommendation system on Zomato's user reviews data for different restaurants. We will be using a technique called Collaborative Filtering which is one of the most popular ways of implementing a recommendation system and we will be training an autoencoder. Don't be afraid if you have never heard these terms before we will be explaining them in detail. -->\n",
    "\n",
    "## Collaborative Filtering\n",
    "\n",
    "Let's say we have ratings of 5 users on 4 different restaurants and a user might not have rated a particular restaurant. We can create arrange the ratings in a user-restaurant matrix from the data, that looks like this:\n",
    "\n",
    "![alt text](data.png)\n",
    "\n",
    "As you can see the users have not rated all the restaurants. In collaborative filtering we try to find these missing values and based on these values we make a recommendation to them. In this figure, for example, Kabir has rated ANC and Looters but not Food King, however Suvigya has rated all three of these restaurants. We can observe that the ratings of Suvigya on ANC and Looters match a lot with Kabir and we can to some extent say that Kabir's rating for Food King will be close to 2. \n",
    "\n",
    "## Autoencoders\n",
    "\n",
    "**Youtube Video:** https://www.youtube.com/watch?v=9zKuYvjFFS8 (till 5:36)\n",
    "\n",
    "An autoencoder is a neural network which learns to copy its input to its output. The network can be viewed as consisting 2 parts, an encoder which downsamples the input to a compact representation and a decoder which produces a reconstruction of the input. Autoencoders falls under the area of unsupervised learning algorithms, since for training an autoencoder we only need raw data without any label. An autoencoder with one hidden layer is shown below:\n",
    "\n",
    "\n",
    "![alt text](autoencoder.png) \n",
    "\n",
    "Please note that we can have as many layers as we want in our autoencoder. The only thing to keep in mind is to reduce the dimensionality of the input in the encoder part of the network and increase the dimensionality of the hidden representation to the size of actual input in decoder network.\n",
    "\n",
    "Once we have defined the architecture of our autoencoder, we can train it using the following steps:\n",
    "\n",
    "1.   Feed the batch of inputs *x* to the network and obtain the predictions *p*\n",
    "2.   Compute the loss between the reconstructed inputs *p* and the actual inputs *x* i.e. *L(p,x)*\n",
    "3. Compute the gradients of this loss with respect to the parameters of the model using backprop.\n",
    "4. Use these gradients to take optimization step, for e.g. Stochastic Gradient Descent.\n",
    "5. Repeat steps 1-4 till convergence.\n",
    "\n",
    "## Collaborative Filtering using Autoencoders\n",
    "\n",
    "We will now see how to use autoencoders for collaborative filtering. Lets first see the steps involved in training an autoencoder for this problem and then we will develope an intuition on why it works.\n",
    "\n",
    "![alt text](autoencoder2.png)\n",
    "\n",
    "In the figure we are feeding Kabir's ratings to the autoencoder. Note that the ratings corresponding to the missing values are kept zero. After computing the predictions of autoencoder we compute mean squared error between the input ratings and the ratings predicting by the network. One important thing to note here is that while computing the loss we do not consider the predictions corresponding to 0 ratings i.e. the missing values and only compute the error for the ratings that we are given. This is because we do not want our network to learn that the output corresponding to the missing values is zero but instead learn a reasonable value. We repeat the same thing for other users and then backprogate the gradients and take optimization steps.\n",
    "\n",
    "During test time we will feed the input with the missing ratings to the network and use the output of the network to determine the ratings of the missing values. Once we have the ratings for those missing values we can make a recommendation accordingly.\n",
    "\n",
    "![alt text](autoencoder3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "abUEaoSCjE-f",
    "nbgrader": {
     "checksum": "29cebb90824aa5122632b2238dd7006e",
     "grade": false,
     "grade_id": "cell-0b9e585d43a72804",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "After reading your notes, you decided to proceed with building the recommendation engine.\n",
    "\n",
    "Fortunately, you don't have to worry about the data collection as your team member Suvigya has already collected the data from Zomato's website in the required format using the code [here](https://github.com/suvigyavijay/zomato-scraper).\n",
    "\n",
    "![alt text](restaurants_data.png)\n",
    "\n",
    "![alt text](review_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "M1vv3u8nt_qi",
    "nbgrader": {
     "checksum": "c63c23b9454a8269ed6a035f486ef104",
     "grade": false,
     "grade_id": "cell-00df6203e24d157d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "After the data collection, another team member, Kabir helped out and extracted the ratings and preprocessed it. Finally, he provided you with the train-test split, in order to ensure that you spent more time on designing the algorithm instead of engineering the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "osxqQarigNPn",
    "nbgrader": {
     "checksum": "edec86f6d57439a68af85d8c2c38764e",
     "grade": false,
     "grade_id": "cell-081723df16cfa8eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Loading necessary packages and defining global variables\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sys import platform\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "OUTPUT_DIR_TRAIN='data/train.dat'\n",
    "OUTPUT_DIR_TEST='data/test1.dat'\n",
    "\n",
    "NUM_RESTS = 5138\n",
    "NUM_USERS = 3579"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "R4DizmM_h0Oq",
    "nbgrader": {
     "checksum": "0069b3e75ea009ec628d82ee5d768889",
     "grade": false,
     "grade_id": "cell-1cd7a8160cc7217d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Restaurant reviews data\n",
    "\n",
    "We have reviews of 5138 restaurants from 3579 different users. Each user is given an ID from 0 to 3578 and similarly, each restaurant is given an ID from 0 to 5137. We have provided you 3 files `train.dat` and `test1.dat` and `test_hidden.dat` containing training data, testing data and hidden test data respectively. For now, do not worry about `train_hidden.dat`. The format of data in the other 2 files is:\n",
    "\n",
    "                        userID, restaurantID, rating\n",
    "                        \n",
    "This means each row contains a rating given to a particular restaurant by a given user. Please have a look at the files and ensure what we said makes sense. The figure below shows how the split has been made from the original data to training and test set.\n",
    "\n",
    "![alt text](datasplit.png)\n",
    "\n",
    "We have removed some ratings from each user at random from train set and added them to test set. This way we can compare the final predictions on the train set and compare with ratings present in the test set, to find out how the network is actually performing.\n",
    "\n",
    "During training, we use the rating data from the `train.dat` file to train our autoencoder. During testing though we feed the data from `train.dat` to the autoencoder and compare the predictions with the values given in `test.dat`. The figure given below demonstrates the testing process.\n",
    "\n",
    "![alt text](testing.png)\n",
    "\n",
    "`test_hidden.dat` only contains the userID and restaurantID and not the ratings. Finally, you will be making predictions for these restaurants, user pairs and will be submitting your predictions to the Kaggle competition. We will then evaluate your predictions against the actual ratings and you will be ranked on the leaderboard accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "7PlmOgogmlnN",
    "nbgrader": {
     "checksum": "39c325d875648c1c139c9d529dcc6309",
     "grade": false,
     "grade_id": "cell-6ac0cf635ccdd4b7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 1: Implement get_sparse_mat function (2 marks)\n",
    "\n",
    "Before we start building our model, we first need to get our data in a suitable form which can be fed to the network. The input to the network will be a vector of size 5138 (number of restaurants) for each user, where each element of the vector will contain the rating of the restaurant whose id matches with the index of that element. If the user has not rated a particular restaurant, the element corresponding to that will be set to zero. \n",
    "\n",
    "For example: If a user has only rated the restaurants with ids 3, 19 and 1009 as 2, 4 and 3.5 respectively, then the feature vector for that user will be a 5138 sized array containing 2 in the index 3, 4 in the index 19, 3.5 in index 1009 and zero everywhere else.\n",
    "\n",
    "Hence our full dataset will be a matrix of size 3579 X 5138 (number of users X number of restaurants).\n",
    "\n",
    "Since we know that the users have reviewed only a small portion of all 5138 restaurants, the data matrix will be very sparse (will mostly contain zeros). Hence it does not make sense to store the whole 3579 X 5138 matrix in memory. Instead, we create a sparse matrix where for each user we store the tuples containing the restaurant id and the rating for that restaurant. \n",
    "\n",
    "For example: In the previous example where we had a user who rated restaurants 3, 19 and 1009 only, we will store the tuples [(3, 2), (19, 4), (1009, 3.5)]. Earlier we store an array of size 5138 elements for the user but now we need to have an array containing 3 elements, saving much memory. While feeding the inputs to the neural net we convert this sparse representation to the full 5138-dimensional vector.\n",
    "\n",
    "The function `get_sparse_mat` takes as the input the filename string which can either be `train.dat` and `test.dat` and constructs a sparse matrix containing the list of tuples for each user as we described above. The output of the function should be a python list of size 3579 with each element being a list of tuples.\n",
    "\n",
    "![alt text](get_sparse_mat.png)\n",
    "\n",
    "**Note 1:** You can read .dat files similar to how you read .txt files. Use python's inbuilt function *open* to create a file pointer lets say *fp*, then you can use functions like read, readline etc. to read the data values from the file. Refer to this [link](https://www.programiz.com/python-programming/file-operation) if you want a refresher to file IO in python.\n",
    "\n",
    "**Note 2:** You can also read .dat files using pandas similar to the way you read CSV files using *pd.read_csv* function.\n",
    "\n",
    "**Note 3:** The tuples in the list (restaurantID, rating) should have restaurantID as an integer value and rating as a float.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Lxobl5WogNP-",
    "nbgrader": {
     "checksum": "4b57d84e6c6c65a139ac0a5747e95c3d",
     "grade": false,
     "grade_id": "cell-cf39d53fdc8c6aa1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_sparse_mat(filename):\n",
    "  \n",
    "    '''\n",
    "    \n",
    "    Inputs: \n",
    "        -filename: a string containing the name of the file from which we want\n",
    "                    to extract the data. In our case it can be either train.dat\n",
    "                    or test.dat\n",
    "                    \n",
    "    Returns a python list of size 3579 (number of users) with each element of\n",
    "    the list being a list of tuples (restaurantID, rating).\n",
    "    \n",
    "    '''\n",
    "    sparse_mat = []\n",
    "    # YOUR CODE HERE\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    df = df.values.tolist()\n",
    "    a = []  # user \n",
    "    b = []  # restro\n",
    "    c =[] # rating given\n",
    "    a_num = int(df[len(df)-1][0]) #number of users \n",
    "    df_trans = list(zip(*df))\n",
    "    a = list(df_trans[0])\n",
    "    b = list(df_trans[1])\n",
    "    for i in range(len(df)):\n",
    "        b[i] = int(b[i])\n",
    "    c = list(df_trans[2])\n",
    "    for i in range(a_num + 1):\n",
    "        f = 0\n",
    "        for j in range(len(df)):\n",
    "            if (df[j][0] == i):\n",
    "                n = j\n",
    "                break\n",
    "        \n",
    "        for j in range(len(df)):\n",
    "            if (df[j][0] == i):\n",
    "                f = f + 1\n",
    "        sparse_mat.append(list(zip(b[n:n+f],c[n:n+f])))\n",
    "    \n",
    "    \n",
    "    return sparse_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "I_KAvnHpgNQB",
    "nbgrader": {
     "checksum": "452258bee1a41fa3ff88ab4068bcbc9a",
     "grade": false,
     "grade_id": "cell-046803b61d6cfe97",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Now that we have implemented the get_sparse_mat function we can get the train and test sparse matrices\n",
    "train_smat = get_sparse_mat(OUTPUT_DIR_TRAIN)\n",
    "test_smat = get_sparse_mat(OUTPUT_DIR_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d2883aabb135bdd2d4613418e3cbce9",
     "grade": false,
     "grade_id": "cell-f2814ee9b5f9f775",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sample Test Case 1\n",
      "Sample Test Case 1 Passed\n",
      "Running Sample Test Case 2\n",
      "Sample Test Case 2 Passed\n",
      "Running Sample Test Case 3\n",
      "Sample Test Case 3 Passed\n",
      "Running Sample Test Case 4\n",
      "Sample Test Case 4 Passed\n",
      "Running Sample Test Case 5\n",
      "Sample Test Case 5 Passed\n",
      "Running Sample Test Case 6\n",
      "Sample Test Case 6 Passed\n",
      "Running Sample Test Case 7\n",
      "Sample Test Case 7 Passed\n",
      "Running Sample Test Case 8\n",
      "Sample Test Case 8 Passed\n"
     ]
    }
   ],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "#SAMPLE TEST CASE\n",
    "print(\"Running Sample Test Case 1\")\n",
    "assert np.allclose(len(train_smat), 3579)\n",
    "print(\"Sample Test Case 1 Passed\")\n",
    "print(\"Running Sample Test Case 2\")\n",
    "assert np.allclose(len(test_smat), 3579)\n",
    "print(\"Sample Test Case 2 Passed\")\n",
    "print(\"Running Sample Test Case 3\")\n",
    "assert np.allclose(len(train_smat[5]), 234)\n",
    "print(\"Sample Test Case 3 Passed\")\n",
    "print(\"Running Sample Test Case 4\")\n",
    "assert np.allclose(train_smat[5][:5],[(626, 4.0), (1177, 4.5), (976, 4.0), (3926, 4.0), (3274, 5.0)])\n",
    "print(\"Sample Test Case 4 Passed\")\n",
    "print(\"Running Sample Test Case 5\")\n",
    "assert np.allclose(len(test_smat[5]),5)\n",
    "print(\"Sample Test Case 5 Passed\")\n",
    "print(\"Running Sample Test Case 6\")\n",
    "assert np.allclose(test_smat[5][:5], [(574, 3.5), (3717, 4.0), (2303, 4.0), (863, 3.5), (1706, 1.0)])\n",
    "print(\"Sample Test Case 6 Passed\")\n",
    "print(\"Running Sample Test Case 7\")\n",
    "assert ((type(train_smat[5][:5][0][0]) is int) and (type(train_smat[5][:5][0][1]) is float))\n",
    "print(\"Sample Test Case 7 Passed\")\n",
    "print(\"Running Sample Test Case 8\")\n",
    "assert ((type(test_smat[5][:5][0][0]) is int) and (type(test_smat[5][:5][0][1]) is float))\n",
    "print(\"Sample Test Case 8 Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "530a3db80df66fb50b4c0fdd341166e1",
     "grade": true,
     "grade_id": "cell-c38d8939b302b1fb",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't change code in this cell\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "#HIDDEN TEST CASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54ca406695247bd0f6ff3045000d7436",
     "grade": false,
     "grade_id": "cell-d1aec5dfd6f1c04a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Dataloaders\n",
    "\n",
    "Next we have defined a dataset class which can be used to efficiently iterate through the dataset. We have provided its implementation for you. Go through it once and make sure you understand it. Using the dataset objects we define data generators for train and test sets which are used to get batches of input data. To learn about how pytorch's Dataset and Dataloader classes work in detail please go through this link: https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "lhoVHE5zgNQO",
    "nbgrader": {
     "checksum": "d5096e55eb1dc96880add179cb6d748b",
     "grade": false,
     "grade_id": "cell-69be83684ed5d6b4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X_sam = torch.zeros(5138)\n",
    "        y_sam = torch.zeros(5138)\n",
    "        for i in range(len(self.X[index])):\n",
    "            X_sam[self.X[index][i][0]] = self.X[index][i][1]\n",
    "\n",
    "        for i in range(len(self.y[index])):\n",
    "            y_sam[self.y[index][i][0]] = self.y[index][i][1]\n",
    "\n",
    "        return X_sam, y_sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "CUzq0fE9gNQQ",
    "nbgrader": {
     "checksum": "f21287a1441a259dba01f43d08dd1928",
     "grade": false,
     "grade_id": "cell-b54acc0bc04f91dc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_smat,train_smat)\n",
    "test_dataset = Dataset(train_smat, test_smat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "xCBvEhgfgNQS",
    "nbgrader": {
     "checksum": "7d71655f31dacd96bb64f9d16ef56551",
     "grade": false,
     "grade_id": "cell-56723c374d90462f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6 if platform == 'linux' else 0}\n",
    "training_generator = data.DataLoader(train_dataset, **params)# sampler = torch.utils.data.SequentialSampler(train_dataset))\n",
    "validation_generator = data.DataLoader(test_dataset, **params)# sampler = torch.utils.data.SequentialSampler(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b86b7dd253db3106870aa47f6e404e32",
     "grade": false,
     "grade_id": "cell-027eaa95f0e4f691",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Implementing Autoencoder Architecture\n",
    "\n",
    "Now that we have our datasets ready we can define the architecture of our autoencoder network. We typically define a network architecture in pytorch by extending the nn.Module class. In the cell below we have demonstrated how you can implement a 3 layer neural network with the following architecture in pytorch.\n",
    "\n",
    "INPUT(size = 100) -> FC+ReLU(size = 64) -> FC+ReLU(size = 32) -> FC(size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "20ce18f4199dec57a6f9aafa23fe4af2",
     "grade": false,
     "grade_id": "cell-9bab307c20e18894",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class threeLayerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        In constructor we define different layers we will use in our architecture.\n",
    "        '''\n",
    "        #Constructor call to the superclass\n",
    "        super(threeLayerNet, self).__init__()\n",
    "        #Defining the layers to be used in the network. \n",
    "        #nn.Linear defines a fully connected layer and the first argument represents the input size and the second represents the output size.\n",
    "        self.layer1 = nn.Linear(100, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 10)\n",
    "        #Defining the activation function to be used in the network\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        The forward function takes passes the input through the layers and returns the output.\n",
    "        Inputs:\n",
    "            -x : Input tensor of shape [N_batch, 100]\n",
    "            \n",
    "        Returns the output of neural network of shape [N_batch, 10]\n",
    "        '''\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.act(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.layer3(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57d35c836179241f3bdff496cfdeacf1",
     "grade": false,
     "grade_id": "cell-378bee8b83d93cad",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0429, -0.1079,  0.2100, -0.0437,  0.0598,  0.0489,  0.2232, -0.1854,\n",
       "         -0.1493, -0.0353],\n",
       "        [ 0.0209, -0.1134,  0.1992, -0.0460,  0.0336,  0.0557,  0.2156, -0.1789,\n",
       "         -0.1579, -0.0424],\n",
       "        [-0.0055, -0.0234,  0.2274, -0.0016,  0.0654,  0.1145,  0.1840, -0.1787,\n",
       "         -0.1968, -0.0328],\n",
       "        [-0.0006, -0.0704,  0.2185, -0.0284,  0.0471,  0.0895,  0.2083, -0.1359,\n",
       "         -0.1475, -0.0158],\n",
       "        [ 0.0019, -0.0558,  0.2171, -0.0243,  0.0607,  0.1144,  0.2093, -0.1387,\n",
       "         -0.1523, -0.0283]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Once we have defined the network class we can create an instance for it.\n",
    "net = threeLayerNet()\n",
    "\n",
    "#To get the output of the network on data just call the network instance and feed the inputs\n",
    "\n",
    "x = torch.rand(5, 100) #Just a random input\n",
    "network_prediction = net(x)\n",
    "network_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c617d2b3f04cc4ff7b57454a354d5bd8",
     "grade": false,
     "grade_id": "cell-fb53aaf110fc5b33",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 2.1 Warmup Excercise: Implement a toy neural network. (1 Mark)\n",
    "\n",
    "Before you implement the autoencoder architecture as a practice implement a simple 2 layer neural with the following architecture:\n",
    "\n",
    "INPUT(size = 224) -> FC+ReLU(size = 128) -> FC+tanh(size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b0e8755de282d098ba187d27a94509b5",
     "grade": false,
     "grade_id": "cell-6d182e2cd570df03",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class twolayerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define the layers and activation functions to be used in the network.\n",
    "        '''\n",
    "        super(twolayerNet, self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        self.layer1 = nn.Linear(224, 128)\n",
    "        self.layer2 = nn.Linear(128, 5)\n",
    "        #Defining the activation function to be used in the network\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.Tanh()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        Implement the forward function which takes as input the tensor x and feeds it to the layers of the network\n",
    "        and returns the output.\n",
    "        \n",
    "        Inputs:\n",
    "            -x : Input tensor of shape [N_batch, 224]\n",
    "            \n",
    "        Returns the output of neural network of shape [N_batch, 5]\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        out = torch.zeros(x.shape[0], 5)\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "                \n",
    "        out = self.layer1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.act2(out)\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8c0ce61ba0bb751ced73775019b37367",
     "grade": false,
     "grade_id": "cell-cc9f5d7a2bc177bd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "net = twolayerNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "17d5b6b37895e8d6b1e3a7a7132a73ee",
     "grade": false,
     "grade_id": "cell-50e1ec89e1826837",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sample Test Case\n",
      "Sample Test Case Passed\n"
     ]
    }
   ],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "    \n",
    "### SAMPLE TEST CASE\n",
    "params_shapes = [p.shape for p in net.parameters()]\n",
    "params_shapes = sorted(params_shapes)\n",
    "print(\"Running Sample Test Case\")\n",
    "assert params_shapes ==[torch.Size([5]),\n",
    " torch.Size([5, 128]),\n",
    " torch.Size([128]),\n",
    " torch.Size([128, 224])]\n",
    "print(\"Sample Test Case Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "897129ade8fd56a7d2d38ede753cbd67",
     "grade": true,
     "grade_id": "cell-4c078de99f5df1b2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't change code in this cell\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "    \n",
    "### HIDDEN TEST CASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d47bf25e8ec61dca6db0d08f0b3102f9",
     "grade": false,
     "grade_id": "cell-54a588d06063e4d4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 2.2: Implement the deep autoencoder (2.5 Marks)\n",
    "\n",
    "Now you will implement the autoencoder network which we will be using to build our recommendation system. The architeture of the network should be:\n",
    "\n",
    "INPUT(size = 5138) -> FC+Tanh(size = 32) -> FC(size = 5138);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "bR2dyOJ7gNQU",
    "nbgrader": {
     "checksum": "1956ace2b07ae51d5528fc7cf9024b7d",
     "grade": false,
     "grade_id": "cell-7b08f5cd0d9e77c4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class DAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define the layers and activation functions to be used in the network.\n",
    "        '''\n",
    "        super(DAE,self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        self.layer1 = nn.Linear(5138,32)\n",
    "        self.layer2 = nn.Linear(32, 5138)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Implement the forward function which takes as input the tensor x and feeds it to the layers of the network\n",
    "        and returns the output.\n",
    "        \n",
    "        Inputs:\n",
    "            -x : Input tensor of shape [N_batch, 5138]\n",
    "            \n",
    "        Returns the output of neural network of shape [N_batch, 5138]\n",
    "        '''\n",
    "        \n",
    "        out = torch.zeros(x.shape[0], 5138)\n",
    "        # YOUR CODE HERE\n",
    "        out = self.layer1(x)\n",
    "        out = self.tanh(out)\n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "CyLsCCpKgNQX",
    "nbgrader": {
     "checksum": "169ab09281f378e7b1dcea6490e31cd6",
     "grade": false,
     "grade_id": "cell-69d680935a26da6c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "net = DAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b751d9b11363e57a3b81c5d9c96effac",
     "grade": false,
     "grade_id": "cell-ab01b51dd537bda5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sample Test Case\n",
      "Sample Test Case Passed\n"
     ]
    }
   ],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "    \n",
    "### SAMPLE TEST CASE\n",
    "params_shapes = [p.shape for p in net.parameters()]\n",
    "params_shapes = sorted(params_shapes)\n",
    "print(\"Running Sample Test Case\")\n",
    "assert params_shapes == [torch.Size([32]), torch.Size([32, 5138]), torch.Size([5138]), torch.Size([5138, 32])]\n",
    "print(\"Sample Test Case Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2cf3bdf8a881695d7ced300342355d39",
     "grade": true,
     "grade_id": "cell-3cdd61c6cf668813",
     "locked": true,
     "points": 2.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't change code in this cell\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "### HIDDEN TEST CASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a414a2c131c45fb48c1a64dd64943774",
     "grade": false,
     "grade_id": "cell-646d884972dae893",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 3: Implement the loss function (2.5 Marks)\n",
    "\n",
    "Now that we have defined our autoencoder network we need to define a loss function to train our model. We will be using mean squared error as our loss function which can be simply implemented by taking the squared sum of the errors between the model predictions and the labels and dividing it by the number of training examples. However, there is a small catch here. As we described in the beginning, for a user we have to compute this error for the restaurants whose ratings have been given by the user and not for the restaurants with the missing ratings.\n",
    "\n",
    "![loss.png](loss.png)\n",
    "\n",
    "Please note that in the figure we are dividing the sum of squared errors with 4 which comes from the total number of ratings that are available in the input data.\n",
    "\n",
    "The function masked_loss takes as the input predictions and labels and calculates the mean squared error for the available ratings. One way of doing this is to first define a mask which is zero for the ratings not available and one for the available ones. Then we multiply this mask with the model predictions so that it zeros out the predictions of the network which are missing in the input data. Now we can calculate the sum of squared errors between the masked predictions and the input ratings and divide it with the number of available ratings which can be calculated by counting the number of ones in the mask.\n",
    "\n",
    "![maskedloss1.png](maskedloss1.png)\n",
    "![maskedloss2.png](maskedloss2.png)\n",
    "![maskedloss3.png](maskedloss3.png)\n",
    "\n",
    "Hint: You might find torch.where function useful in creating the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "kNtI-Z3BgNQZ",
    "nbgrader": {
     "checksum": "e7326b8c37919cdecdbaafc1cd6b4a1d",
     "grade": false,
     "grade_id": "cell-f6aaae25b10cb2eb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def masked_loss(preds, labels):\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "        -preds: Model predictions [N_batch, 5138]\n",
    "        -labels: User ratings [N_batch, 5138]\n",
    "        \n",
    "    Returns the masked loss as described above.\n",
    "    '''\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    labels = labels.clone().detach()\n",
    "    onestensor = torch.ones(len(labels),5138)\n",
    "    zerostensor = torch.zeros(len(labels),5138)\n",
    "    mask = torch.where(labels>0,onestensor,zerostensor)\n",
    "    total_number = torch.sum(mask).item()\n",
    "    actual_pred = preds*mask\n",
    "    loss = torch.sum((labels - actual_pred)**2)\n",
    "    \n",
    "    loss = loss/total_number\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cc135435ef4774cdd562f25904f13e42",
     "grade": false,
     "grade_id": "cell-a620e81451705a29",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sample Test Case\n",
      "Sample Test Case Passed\n"
     ]
    }
   ],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "    \n",
    "### SAMPLE TEST CASE\n",
    "x = torch.zeros(3, 5138)\n",
    "x[0][100] = 1\n",
    "x[0][7] = 1\n",
    "x[0][1009] = 1\n",
    "x[1][101] = 1\n",
    "x[1][8] = 1\n",
    "x[1][1010] = 1\n",
    "x[1][56] = 1\n",
    "x[2][102] = 1\n",
    "x[2][9] = 1\n",
    "loss = masked_loss(net(x), x).item()\n",
    "print(\"Running Sample Test Case\")\n",
    "assert np.allclose(loss, 1.1857765913009644, atol = 1e-4)\n",
    "print(\"Sample Test Case Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40959e518967e9a6a71cef465208b47d",
     "grade": true,
     "grade_id": "cell-830157d8d2f759fa",
     "locked": true,
     "points": 2.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't change code in this cell\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                            \"\"\"Don't change code in this cell\"\"\"\n",
    "    \n",
    "### HIDDEN TEST CASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "987ada0ec3a405b513100e6c1efa6bba",
     "grade": false,
     "grade_id": "cell-527133cfdf013836",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Defining the Optimizer\n",
    "\n",
    "Now that we have our autoencoder architecture and loss function ready we only need to define an optimizer object before we begin training. We will be using Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.1. Pytorch comes with optim module which contains different optimizers for optimizing neural nets. We can define an instance of an optimizer as:\n",
    "optim.OptimizerName(net.parameters(), learning_rate), where the first argument corresponds to the list of parameters of the network and the second parameter being the learning rate. We have defined SGD optimizer object in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "V3kqoSF9gNQg",
    "nbgrader": {
     "checksum": "89ea1aab370049714e6d70818c701b32",
     "grade": false,
     "grade_id": "cell-2b594031f6db722f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "opti = optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9a698106db05852cee7584ba6c2834ea",
     "grade": false,
     "grade_id": "cell-8caf5c05eee786bb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 4: Training the model (2 marks)\n",
    "\n",
    "Now we have everything ready to start training our model. Each iteration will consist of 4 important steps:\n",
    "\n",
    "1. Feeding input data to the network and obtaining model predictions\n",
    "2. Compute the loss between inputs and labels say *loss*\n",
    "3. Backpropagate the gradients using *loss.backward()*\n",
    "4. Take the optimization step using the *step* method of the optimizer instance.\n",
    "\n",
    "This function will be manually graded since there the training results might vary each time we run the network. Effectively you should see a decrease in both train and validation losses. The final training loss should be around 1.7 and validation loss should be around 2 to 2.2 towards the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "zYZnTbjIgNQv",
    "nbgrader": {
     "checksum": "971183cb24dcf0c0f3557918f46952a8",
     "grade": false,
     "grade_id": "cell-919c9ecf6bab12d3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "outputId": "5bb2ec97-ae30-4b5c-dc4d-4c8d3db0f999"
   },
   "outputs": [],
   "source": [
    "def train(net, criterion, opti, training_generator, validation_generator, max_epochs = 10):\n",
    "    \n",
    "    '''\n",
    "    Inputs:\n",
    "        - net: The model instance\n",
    "        - criterion: Loss function, in our case it is masked_loss function.\n",
    "        - opti: Optimizer Instance\n",
    "        - training_generator: For iterating through the training set\n",
    "        - validation_generator: For iterating through the test set\n",
    "        - max_epochs: Number of training epochs. One epoch is defined as one complete presentation of the data set.\n",
    "    \n",
    "    Outputs:\n",
    "        - train_losses: a list of size max_epochs containing the average loss for each epoch of training set.\n",
    "        - val_losses: a list of size max_epochs containing the average loss for each epoch of test set.\n",
    "        \n",
    "        Note: We compute the average loss in an epoch by summing the loss at each iteration of that epoch\n",
    "        and then dividing the sum by the number of iterations in that epoch.\n",
    "    '''\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        running_loss = 0 #Accumulate the loss in each iteration of the epoch in this variable\n",
    "        cnt = 0 #Increment it each time to find the number iterations in the epoch.\n",
    "        # Training iterations\n",
    "        for batch_X, batch_y in training_generator:\n",
    "            opti.zero_grad() #Clears the gradients of all variables.\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            output = net(batch_X)\n",
    "            loss = criterion(output,batch_y)\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            running_loss = running_loss + loss.item()\n",
    "            cnt = cnt+1\n",
    "            #raise NotImplementedError()\n",
    "\n",
    "        print(\"Epoch {}: Training Loss {}\".format(epoch+1, running_loss/cnt))\n",
    "        train_losses.append(running_loss/cnt)\n",
    "        \n",
    "        \n",
    "        #Now that we have trained the model for an epoch, we evaluate it on the test set\n",
    "        running_loss = 0\n",
    "        cnt = 0\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for batch_X, batch_y in validation_generator:\n",
    "                output = net(batch_X)\n",
    "                loss = criterion(output,batch_y)\n",
    "                running_loss = running_loss + loss.item()\n",
    "                cnt = cnt+1\n",
    "                \n",
    "\n",
    "                # YOUR CODE HERE\n",
    "                #raise NotImplementedError()\n",
    "                \n",
    "        print(\"Epoch {}: Validation Loss {}\".format(epoch+1, running_loss/cnt))\n",
    "\n",
    "        val_losses.append(running_loss/cnt)\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "64734d27e956c01de44fac3b4ac069ce",
     "grade": false,
     "grade_id": "cell-9a0fc7443ea41b0e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss 14.697319626808167\n",
      "Epoch 1: Validation Loss 13.367216399737767\n",
      "Epoch 2: Training Loss 11.468891228948321\n",
      "Epoch 2: Validation Loss 10.564270496368408\n",
      "Epoch 3: Training Loss 9.08431670495442\n",
      "Epoch 3: Validation Loss 8.605725824832916\n",
      "Epoch 4: Training Loss 7.458766145365579\n",
      "Epoch 4: Validation Loss 7.225853034428188\n",
      "Epoch 5: Training Loss 6.28293833562306\n",
      "Epoch 5: Validation Loss 6.2190693361418585\n",
      "Epoch 6: Training Loss 5.416922960962568\n",
      "Epoch 6: Validation Loss 5.4537621055330545\n",
      "Epoch 7: Training Loss 4.7521442260060995\n",
      "Epoch 7: Validation Loss 4.8547365580286295\n",
      "Epoch 8: Training Loss 4.228861038173948\n",
      "Epoch 8: Validation Loss 4.374872403485434\n",
      "Epoch 9: Training Loss 3.800852732998984\n",
      "Epoch 9: Validation Loss 3.9852977863379886\n",
      "Epoch 10: Training Loss 3.449286869594029\n",
      "Epoch 10: Validation Loss 3.6614114897591725\n",
      "Epoch 11: Training Loss 3.165797999926976\n",
      "Epoch 11: Validation Loss 3.387877391917365\n",
      "Epoch 12: Training Loss 2.9152906792504445\n",
      "Epoch 12: Validation Loss 3.158147475549153\n",
      "Epoch 13: Training Loss 2.704080253839493\n",
      "Epoch 13: Validation Loss 2.9585662313870023\n",
      "Epoch 14: Training Loss 2.5237160452774594\n",
      "Epoch 14: Validation Loss 2.7863969781569073\n",
      "Epoch 15: Training Loss 2.368889412709645\n",
      "Epoch 15: Validation Loss 2.6353308813912526\n",
      "Epoch 16: Training Loss 2.230725264975003\n",
      "Epoch 16: Validation Loss 2.501964728747095\n",
      "Epoch 17: Training Loss 2.109004855155945\n",
      "Epoch 17: Validation Loss 2.3852547875472476\n",
      "Epoch 18: Training Loss 2.0010699778795242\n",
      "Epoch 18: Validation Loss 2.2784234327929362\n",
      "Epoch 19: Training Loss 1.9048666379281454\n",
      "Epoch 19: Validation Loss 2.1849134138652255\n",
      "Epoch 20: Training Loss 1.8146920140300478\n",
      "Epoch 20: Validation Loss 2.1002215849501744\n"
     ]
    }
   ],
   "source": [
    "net = DAE()\n",
    "opti = optim.SGD(net.parameters(), lr = 1e-1)\n",
    "train_losses, val_losses = train(net, masked_loss, opti, training_generator, validation_generator, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss 14.781565036092486\n",
      "Epoch 1: Validation Loss 13.64286208152771\n",
      "Epoch 2: Training Loss 11.752452424594335\n",
      "Epoch 2: Validation Loss 10.907969917569842\n",
      "Epoch 3: Training Loss 9.348624902112144\n",
      "Epoch 3: Validation Loss 8.85697603225708\n",
      "Epoch 4: Training Loss 7.649637162685394\n",
      "Epoch 4: Validation Loss 7.391495909009661\n",
      "Epoch 5: Training Loss 6.416823336056301\n",
      "Epoch 5: Validation Loss 6.321943870612553\n",
      "Epoch 6: Training Loss 5.496676317283085\n",
      "Epoch 6: Validation Loss 5.520362394196646\n",
      "Epoch 7: Training Loss 4.814222931861877\n",
      "Epoch 7: Validation Loss 4.898396330220359\n",
      "Epoch 8: Training Loss 4.266260253531592\n",
      "Epoch 8: Validation Loss 4.406876091446195\n",
      "Epoch 9: Training Loss 3.8374381363391876\n",
      "Epoch 9: Validation Loss 4.00796103477478\n",
      "Epoch 10: Training Loss 3.4774081025804793\n",
      "Epoch 10: Validation Loss 3.6805520951747894\n",
      "Epoch 11: Training Loss 3.181366068976266\n",
      "Epoch 11: Validation Loss 3.406405508518219\n",
      "Epoch 12: Training Loss 2.9326026013919284\n",
      "Epoch 12: Validation Loss 3.1734159333365306\n",
      "Epoch 13: Training Loss 2.729302244526999\n",
      "Epoch 13: Validation Loss 2.9722628721169064\n",
      "Epoch 14: Training Loss 2.5396477367196764\n",
      "Epoch 14: Validation Loss 2.8006679671151296\n",
      "Epoch 15: Training Loss 2.3820995134966716\n",
      "Epoch 15: Validation Loss 2.650042074067252\n",
      "Epoch 16: Training Loss 2.243972580347742\n",
      "Epoch 16: Validation Loss 2.5169520867722377\n",
      "Epoch 17: Training Loss 2.1233865874154225\n",
      "Epoch 17: Validation Loss 2.3989648265498027\n",
      "Epoch 18: Training Loss 2.017589720232146\n",
      "Epoch 18: Validation Loss 2.2928161025047302\n",
      "Epoch 19: Training Loss 1.9178883816514696\n",
      "Epoch 19: Validation Loss 2.198574436562402\n",
      "Epoch 20: Training Loss 1.828224744115557\n",
      "Epoch 20: Validation Loss 2.1135454263005937\n",
      "Epoch 21: Training Loss 1.7516935957329614\n",
      "Epoch 21: Validation Loss 2.0364851696150645\n",
      "Epoch 22: Training Loss 1.6827490031719208\n",
      "Epoch 22: Validation Loss 1.966118899839265\n",
      "Epoch 23: Training Loss 1.6188944556883402\n",
      "Epoch 23: Validation Loss 1.9012192551578795\n",
      "Epoch 24: Training Loss 1.5594875706093652\n",
      "Epoch 24: Validation Loss 1.842172228864261\n",
      "Epoch 25: Training Loss 1.5026286776576723\n",
      "Epoch 25: Validation Loss 1.788136186344283\n",
      "Epoch 26: Training Loss 1.4537090552704675\n",
      "Epoch 26: Validation Loss 1.739068159035274\n",
      "Epoch 27: Training Loss 1.4092255724327905\n",
      "Epoch 27: Validation Loss 1.6931152535336358\n",
      "Epoch 28: Training Loss 1.3651977947780065\n",
      "Epoch 28: Validation Loss 1.6502265248979842\n",
      "Epoch 29: Training Loss 1.3282819432871682\n",
      "Epoch 29: Validation Loss 1.6101478167942591\n",
      "Epoch 30: Training Loss 1.291028693318367\n",
      "Epoch 30: Validation Loss 1.573778891137668\n",
      "Epoch 31: Training Loss 1.256169502224241\n",
      "Epoch 31: Validation Loss 1.539979419537953\n",
      "Epoch 32: Training Loss 1.225445915545736\n",
      "Epoch 32: Validation Loss 1.507854578750474\n",
      "Epoch 33: Training Loss 1.199244784457343\n",
      "Epoch 33: Validation Loss 1.476925043123109\n",
      "Epoch 34: Training Loss 1.1704412283641952\n",
      "Epoch 34: Validation Loss 1.4486811991248811\n",
      "Epoch 35: Training Loss 1.1461010075041227\n",
      "Epoch 35: Validation Loss 1.4219305643013544\n",
      "Epoch 36: Training Loss 1.121261099619525\n",
      "Epoch 36: Validation Loss 1.397036924958229\n",
      "Epoch 37: Training Loss 1.099839364843709\n",
      "Epoch 37: Validation Loss 1.3731553799339704\n",
      "Epoch 38: Training Loss 1.0791033240301269\n",
      "Epoch 38: Validation Loss 1.3513446833406175\n",
      "Epoch 39: Training Loss 1.0564775466918945\n",
      "Epoch 39: Validation Loss 1.3301872770701135\n",
      "Epoch 40: Training Loss 1.0386905010257448\n",
      "Epoch 40: Validation Loss 1.3096768015197344\n",
      "Epoch 41: Training Loss 1.0225780457258224\n",
      "Epoch 41: Validation Loss 1.2904677391052246\n",
      "Epoch 42: Training Loss 1.0040465827499117\n",
      "Epoch 42: Validation Loss 1.273111759551934\n",
      "Epoch 43: Training Loss 0.9895477316209248\n",
      "Epoch 43: Validation Loss 1.2558283401387078\n",
      "Epoch 44: Training Loss 0.9749097696372441\n",
      "Epoch 44: Validation Loss 1.2391872395362173\n",
      "Epoch 45: Training Loss 0.95943169934409\n",
      "Epoch 45: Validation Loss 1.2238851583429746\n",
      "Epoch 46: Training Loss 0.945697403379849\n",
      "Epoch 46: Validation Loss 1.2087801141398293\n",
      "Epoch 47: Training Loss 0.9332688876560756\n",
      "Epoch 47: Validation Loss 1.1950686478189059\n",
      "Epoch 48: Training Loss 0.9209722269858632\n",
      "Epoch 48: Validation Loss 1.1817507201007433\n",
      "Epoch 49: Training Loss 0.9087379212890353\n",
      "Epoch 49: Validation Loss 1.1686566833938872\n",
      "Epoch 50: Training Loss 0.898700336260455\n",
      "Epoch 50: Validation Loss 1.1561258254306657\n",
      "Epoch 51: Training Loss 0.8865706058485168\n",
      "Epoch 51: Validation Loss 1.1444902579699243\n",
      "Epoch 52: Training Loss 0.8760921987039703\n",
      "Epoch 52: Validation Loss 1.1337726329054152\n",
      "Epoch 53: Training Loss 0.8692309930920601\n",
      "Epoch 53: Validation Loss 1.1228396860616547\n",
      "Epoch 54: Training Loss 0.8589405066200665\n",
      "Epoch 54: Validation Loss 1.1119886658021383\n",
      "Epoch 55: Training Loss 0.8487880794065339\n",
      "Epoch 55: Validation Loss 1.101954239819731\n",
      "Epoch 56: Training Loss 0.8407658562064171\n",
      "Epoch 56: Validation Loss 1.093277143580573\n",
      "Epoch 57: Training Loss 0.8318567659173693\n",
      "Epoch 57: Validation Loss 1.0832866792167937\n",
      "Epoch 58: Training Loss 0.8254938551357814\n",
      "Epoch 58: Validation Loss 1.075049347111157\n",
      "Epoch 59: Training Loss 0.8170971348881721\n",
      "Epoch 59: Validation Loss 1.0663648662822587\n",
      "Epoch 60: Training Loss 0.8101207443646022\n",
      "Epoch 60: Validation Loss 1.0580948080335344\n",
      "Epoch 61: Training Loss 0.8031273250068937\n",
      "Epoch 61: Validation Loss 1.0500084300126349\n",
      "Epoch 62: Training Loss 0.7957950755953789\n",
      "Epoch 62: Validation Loss 1.042358636856079\n",
      "Epoch 63: Training Loss 0.7887201021824565\n",
      "Epoch 63: Validation Loss 1.035351921405111\n",
      "Epoch 64: Training Loss 0.7827043991003718\n",
      "Epoch 64: Validation Loss 1.0279957854322024\n",
      "Epoch 65: Training Loss 0.776112266949245\n",
      "Epoch 65: Validation Loss 1.021077573299408\n",
      "Epoch 66: Training Loss 0.7704886219331196\n",
      "Epoch 66: Validation Loss 1.0148018492119653\n",
      "Epoch 67: Training Loss 0.7661916060107095\n",
      "Epoch 67: Validation Loss 1.0082235346947397\n",
      "Epoch 68: Training Loss 0.7605214416980743\n",
      "Epoch 68: Validation Loss 1.0022520571947098\n",
      "Epoch 69: Training Loss 0.7549132949539593\n",
      "Epoch 69: Validation Loss 0.995742529630661\n",
      "Epoch 70: Training Loss 0.7501557618379593\n",
      "Epoch 70: Validation Loss 0.9902183817965644\n",
      "Epoch 71: Training Loss 0.7460453648652349\n",
      "Epoch 71: Validation Loss 0.9842725587742669\n",
      "Epoch 72: Training Loss 0.7397875615528652\n",
      "Epoch 72: Validation Loss 0.9789310917258263\n",
      "Epoch 73: Training Loss 0.7350710130163601\n",
      "Epoch 73: Validation Loss 0.9738795182534626\n",
      "Epoch 74: Training Loss 0.730031176337174\n",
      "Epoch 74: Validation Loss 0.9680798298546246\n",
      "Epoch 75: Training Loss 0.7262337175863129\n",
      "Epoch 75: Validation Loss 0.9634354955383709\n",
      "Epoch 76: Training Loss 0.7210911512374878\n",
      "Epoch 76: Validation Loss 0.958474657365254\n",
      "Epoch 77: Training Loss 0.7169506092156682\n",
      "Epoch 77: Validation Loss 0.9534136354923248\n",
      "Epoch 78: Training Loss 0.7141703558819634\n",
      "Epoch 78: Validation Loss 0.9491535308105605\n",
      "Epoch 79: Training Loss 0.7099812743919236\n",
      "Epoch 79: Validation Loss 0.944748078073774\n",
      "Epoch 80: Training Loss 0.7051291135804993\n",
      "Epoch 80: Validation Loss 0.9401294567755291\n",
      "Epoch 81: Training Loss 0.701749151306493\n",
      "Epoch 81: Validation Loss 0.9359411128929683\n",
      "Epoch 82: Training Loss 0.6979160894240651\n",
      "Epoch 82: Validation Loss 0.9320545313613755\n",
      "Epoch 83: Training Loss 0.6946466277752604\n",
      "Epoch 83: Validation Loss 0.927791144166674\n",
      "Epoch 84: Training Loss 0.6907841286488942\n",
      "Epoch 84: Validation Loss 0.9237340262957981\n",
      "Epoch 85: Training Loss 0.6863028896706445\n",
      "Epoch 85: Validation Loss 0.9200508573225566\n",
      "Epoch 86: Training Loss 0.6828557082584926\n",
      "Epoch 86: Validation Loss 0.9165283931153161\n",
      "Epoch 87: Training Loss 0.6808439525110381\n",
      "Epoch 87: Validation Loss 0.9123444748776299\n",
      "Epoch 88: Training Loss 0.6772048675588199\n",
      "Epoch 88: Validation Loss 0.9087100220578057\n",
      "Epoch 89: Training Loss 0.6738535644752639\n",
      "Epoch 89: Validation Loss 0.9055205509066582\n",
      "Epoch 90: Training Loss 0.6697672020111766\n",
      "Epoch 90: Validation Loss 0.9017911297934396\n",
      "Epoch 91: Training Loss 0.6677670372383935\n",
      "Epoch 91: Validation Loss 0.8986141883901188\n",
      "Epoch 92: Training Loss 0.6666186164532389\n",
      "Epoch 92: Validation Loss 0.8955846641744886\n",
      "Epoch 93: Training Loss 0.6616845290575709\n",
      "Epoch 93: Validation Loss 0.8922098747321537\n",
      "Epoch 94: Training Loss 0.6589751530970845\n",
      "Epoch 94: Validation Loss 0.8891703954764775\n",
      "Epoch 95: Training Loss 0.65714330971241\n",
      "Epoch 95: Validation Loss 0.8865781692521912\n",
      "Epoch 96: Training Loss 0.6530789317829269\n",
      "Epoch 96: Validation Loss 0.8833945702229228\n",
      "Epoch 97: Training Loss 0.6504042936222894\n",
      "Epoch 97: Validation Loss 0.8802739211491176\n",
      "Epoch 98: Training Loss 0.6484275609254837\n",
      "Epoch 98: Validation Loss 0.8774878212383815\n",
      "Epoch 99: Training Loss 0.646166493850095\n",
      "Epoch 99: Validation Loss 0.8742777683905193\n",
      "Epoch 100: Training Loss 0.6435212822897094\n",
      "Epoch 100: Validation Loss 0.8719082825950214\n"
     ]
    }
   ],
   "source": [
    "#with this training loss was reduced to around .7 and training error was redcued to .9. \n",
    "net = DAE()\n",
    "opti = optim.SGD(net.parameters(), lr = 1e-1)\n",
    "train_losses, val_losses = train(net, masked_loss, opti, training_generator, validation_generator, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "kFWYWx9SgNQx",
    "nbgrader": {
     "checksum": "8f6039be4aa4034a85293e1578006d73",
     "grade": false,
     "grade_id": "cell-eb57fe16e29cfd8c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "4fc46a34-eb10-4d4c-8fea-552b4b57ce98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epochs')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally we plot the graphs for loss vs epochs.\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38fd6c8563bbb964934568a128e5f170",
     "grade": false,
     "grade_id": "cell-f5defeaa9ece7662",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Lets see how the network predictions compare with the actual ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "YeaZPgCMgNQ2",
    "nbgrader": {
     "checksum": "479131ddb8675ea6c7e91db52667264d",
     "grade": false,
     "grade_id": "cell-4e6ab6b3fb2269ff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Ratings:  [3.7889006 3.462549  3.3646958 3.921911  4.1797633]\n",
      "Actual Ratings:  [3.5 3.5 4.  4.  4. ]\n"
     ]
    }
   ],
   "source": [
    "x, y = test_dataset.__getitem__(4)\n",
    "pred = net(x)\n",
    "print(\"Predicted Ratings: \", pred[y!=0].detach().numpy())\n",
    "print(\"Actual Ratings: \", y[y!=0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d18b64a0c7d7686230b97938da45fb6d",
     "grade": false,
     "grade_id": "cell-6ffa3a768c1d8f7e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 5: Improving the model (10 marks)\n",
    "\n",
    "From here onwards the assignment becomes open-ended. We have so far implemented a very basic autoencoder for this task. We obtained MSE in the range 2-2.2 on the test data which is still a very huge error and not acceptable in practice. However, there are many changes that we can make in our architecture and learning algorithm to improve its performance. For example:\n",
    "\n",
    "1. Deeper architecture, different activation functions and different hidden sizes particularly for the middle layer\n",
    "2. Using Dropout or L2 regularization to reduce overfitting\n",
    "3. Using Batch norm to improve training\n",
    "4. Different optimizers like AdaDelta, RMSProp, Adam etc etc\n",
    "5. Tuning hyperparameters like learning rate, l2 regularization constant, dropout rate etc\n",
    "6. Using a different loss function like mean absolute error.\n",
    "7. Creating model ensemble i.e. training multiple neural nets and then combining the outputs of all the networks to get the final output\n",
    "8. Different algorithm all together like Variational Autoencoders, SVD etc\n",
    "9. Train for more number of epochs, here we only trained for 20 epochs maybe more epochs can help\n",
    "10. Early Stopping, learning rate decay\n",
    "\n",
    "After you have made the changes use your final model to get predictions on all the users using the training data. We have provided you with a file `test_hidden.dat` which contains data in the form `userId, restaurantID`. The user ratings for these restaurantIDs are missing in the training set. From our model predictions we will obtain the ratings corresponding to these restaurants and store them in file `predictions.csv` in the format `prediction` for corresponding `userId, restaurantID` in a predefined sequence. You are then required to upload this CSV file on the kaggle competition created for this assignment along with this notebook.\n",
    "\n",
    "Below we have given a function that you can run after training your final model to generate the `predictions.csv` file. You might need to change the function a little according to the way you take predictions from your model. This function assumes that you have a pytorch model instance *net* as your final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(net, train_data = train_smat):\n",
    "    \n",
    "    def get_test_smat(filename = 'data/test_hidden.dat'):\n",
    "        sparse_dict = defaultdict(list)\n",
    "        for line in open(filename):\n",
    "            splitted_line = line.split(',')\n",
    "            sparse_dict[int(splitted_line[0])].append((int(splitted_line[1])))\n",
    "\n",
    "        sparse_mat = []\n",
    "        sKeys = sorted(sparse_dict)\n",
    "        for key in sKeys:\n",
    "            sparse_mat.append(sparse_dict[key])\n",
    "        \n",
    "        return sparse_mat\n",
    "            \n",
    "            \n",
    "    test_smat = get_test_smat()\n",
    "    preds = []\n",
    "    for i in range(len(train_data)):\n",
    "        \n",
    "        #Getting the actual vector from the sparse representation\n",
    "        x = torch.zeros(5138)\n",
    "        for j in range(len(train_data[i])):\n",
    "            x[train_data[i][j][0]] = train_data[i][j][1]\n",
    "        with torch.set_grad_enabled(False):\n",
    "            pred = net(x).detach().numpy() ## This logic might be different for your model, change this accordingly\n",
    "        \n",
    "        pred = pred[test_smat[i]]\n",
    "        user_rest_pred = np.concatenate([i*np.ones((len(pred),1),dtype=np.int),np.array(test_smat[i],dtype=np.int)[:,None], np.array(pred)[:,None]],axis = 1)\n",
    "        preds += user_rest_pred.tolist()\n",
    "        \n",
    "    preds = np.array(preds)\n",
    "    df = pd.DataFrame(preds)\n",
    "    df[0] = df[0].astype('int')\n",
    "    df[1] = df[1].astype('int')\n",
    "    df[2] = df[2].astype('float16')\n",
    "    df = df.drop(df.columns[[0, 1]], axis=1)\n",
    "    df.to_csv('predictions.csv', index = True, header = False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.367188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.964844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.894531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.441406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.019531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          2\n",
       "0  4.367188\n",
       "1  3.964844\n",
       "2  3.894531\n",
       "3  4.441406\n",
       "4  4.019531"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_predictions(net)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predictions.csv` needs to be uploaded on the Kaggle, instructions for which will be communicated through mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35790"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
